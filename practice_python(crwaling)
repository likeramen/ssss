# import urllib.request as ureq
#
# url = "https://book.naver.com/"
# response = ureq.urlopen(url)
# code = response.getcode()                                           # 응답코드 받아오기
# print(code)
#
# # url = "https://www.python.org/static/img/python-logo.png"
# # savename = 'test1.png'                                            # 저장될 이미지 이름
# # image_png = ureq.urlopen(url).read()                              # 이미지 받아오기
# # with open(savename, 'wb')  as f:
# #     f.write(image_png)                                            # 이미지 저장시킴
#
# # url = "https://www.python.org/static/img/python-logo.png"
# # savename = 'test2.png'
# # ureq.urlretrieve(url, savename)                                     # 이렇게 간단하게 저장시킬 수 있음
#
# url = "https://book.naver.com/"
# response = ureq.urlopen(url)
# html_data = response.read().decode('utf-8')                           # 소스코드 받아오기. 디코딩 안하면 바이너리 타입으로 반환 됨.
# print(html_data)

# import urllib.parse as upa
#
# query = {'user':'홍 길동'}
# data = upa.urlencode(query)
# print(data)
#
# print(upa.urlparse("http://www.minhaett.net/me?id=minhaett"))

# import urllib.request as ureq
#
# url = "https://www.wevity.com/"
# response = ureq.urlopen(url)
# code = response.getcode()
# print(code)
#
# url = "https://www.wevity.com/images/logo_wevity.png"
# savename = "wevity.png"
# ureq.urlretrieve(url, savename)
#
# url = "https://www.naver.com/"
# response = ureq.urlopen(url)
# html_data = response.read().decode('utf-8')
# print(html_data)

import requests                                                                 # requests 라이브러리가 쓰기 편함.

# url = "https://naver.com"
# response = requests.get(url)
# print(response)                                                                 # 응답 받아오기

# url = "https://book.naver.com/search/search.nhn"
# params = {'quesry' : '파이썬'}
# response = requests.get(url, params=params)
# print(response.url)                                                             # url에 get방식으로 params옵션 붙여주기
#
# url = "https://naver.com"
# headers = {"Contest-Type" : "application/json; charset=utf-8"}
# response = requests.get(url, headers=headers)                                   # get방식으로 hearder에 넣기 주소창으로는 확이 불가 개발자 도구로 해야함.
# print(response.url)
#
# url = "https://naver.com"
# cookies = {'session_id' : 'minhaett'}
# response = requests.get(url, cookies=cookies)                                   # get방식으로 쿠키옵션 넣기
# print(response.url)
#
# url = "https://naver.com"
# response = requests.post(url)                                                   # post방식으로 응답 받아오기
# print(response)
#
# url = "https://naver.com"
# data = {'id' : 'minhaett'}
# response = requests.post(url, data=data)                                        # post방식에서는 params이 아닌 data다
# print(response.url)
#
# url = "https://naver.com"
# response = requests.put(url)                                                    # put메소드 -> put메소드는 클라이언트가 서버로 정보를 전송하고자 할 때 사용
# print(response)
#
# url = "https://naver.com"
# response = requests.delete(url)                                                 # delete메소드 응답 -> 요청한 URI의 리소스를 삭제, 서버에 놓여 있는 파일을 지울 때
# print(response)

session_use = requests.session()                                                  # session 메소드 -> 클라이언트와 서버간의 접속을 유지
url = "https://naver.com/"
response = session_use.get(url)
print(response)

url = "https://naver.com"
response = requests.get(url)
code = response.status_code                                                       # 응답 코드 호출
print(code)

url = "http://finance.naver.com"
response = requests.get(url)                                                      # 인코딩 방식 호출
print(response.encoding)

url = "https://book.naver.com/search/search.nhn?query=파이썬"
response = requests.get(url)
html_data = response.text                                                         # 코드를 텍스트 형식으로 호출
print(html_data)

url = "https://book.naver.com/search/search.nhn?query=파이썬"
response = requests.get(url)
html_data = response.content                                                      # 코드를 바이너리 형식으로 호출
print(html_data)

url = "https://www.python.org/static/img/python-logo.png"
response = requests.get(url)
with open('test.png', 'wb') as f:                                                 # 이미지 파일 다운
    f.write(response.content)

from bs4 import BeautifulSoup
html = """
<html>
<body>
<h1>객체 지향 언어</h1>
<p> 클래스 기반의 언어 </p>
<p> 프로토타입 기반의 언어 </p>
</body>
</html>
"""

soup = BeautifulSoup(html, "html.parser")
h1 = soup.html.body.h1
p1 = soup.html.body.p
p2 = p1.next_sibling.next_sibling
print(h1)
print(p1)
print(p2)
print(h1.string, p1.string, p2.string)

import re
html = """
<ul>
    <li><a href = "https://minhaett.net/java">java</li>
    <li><a href = "https://minhaett.net/python">python</li>
    <li><a href = "https://minhaett.net/spring">Spring Framework</li>
</ul>
"""

soup = BeautifulSoup(html, 'html.parser')
li = soup.find_all(href=re.compile(r'^https://'))
for i in li:
    print(i.attrs['href'])





from bs4 import BeautifulSoup
import urllib.request as req

url = "https://finance.naver.com/marketindex/"
html = req.urlopen(url)
soup = BeautifulSoup(html, 'html.parser')

price = soup.select_one('#exchangeList > li.on > a.head.usd > div > span.value').string

change = soup.select_one('#exchangeList > li > a.head.usd > div > span.change').string

blind = soup.select_one('#exchangeList > li > a.head.usd > div > span.blind').string

print('미국USD 환전고시환율의 현재 가격은 {}원입니다.'.format(price))
print('미국USD 환전고시환율의 변동 가격은 {}원입니다.'.format(change))
print('미국USD 환전고시환율은 {}입니다.'.format(blind))


from bs4 import BeautifulSoup

html = """
<html>
<body>
 <p>언어이므로 반복만이 실력이 향상된다.</p>
 <h1 id="title">객체지향 언어는</h1>
 <p>클래스 기반의 언어와 프로토타입 기반의 언어가 있다.</p>
</body>
</html>
"""

soup = BeautifulSoup(html, 'lxml')
title = soup.find(id='title')
p = soup.find('p')
print(title.string, p.string)

html = """
<html>
<body>
    <ul>
        <li><a href="https://www.naver.com">naver</a></li>
        <li><a href="https://www.daum.net">daum</a></li>
    </ul>
</body>
</html>
"""

soup = BeautifulSoup(html, 'html.parser')
links = soup.find_all('a')
for a in links:
    href = a.attrs['href']
    text = a.string
    print(text, ':', href)

html = """
<html>
<body>
    <p><a href="https://www.naver.com">naver</a></p>
</body>
</html>
"""

soup = BeautifulSoup(html, 'lxml')
print(soup.prettify())

html = """
<html>
<body>
    <p><a href="htttps://www.naver.com">naver</a></p>
</body>
</html>
"""

soup = BeautifulSoup(html, 'lxml')
a = soup.p.a
print(type(a.attrs))
href = 'href' in a.attrs
print(href)
print(a['href'])

html = """
<html>
<body>
<div id="bigdata">
 <h3>빅데이터의 요소</h3>
 <ul>
 <li>크기(Volume)</li>
 <li>다양성(Variety)</li>
 <li>속도(Velocity)</li>
 <li>정확성(Veracity)</li>
 <li>가치(Value)</li>
 </ul>
</div>
</body>
</html>
"""

soup = BeautifulSoup(html, 'html.parser')
h3 = soup.select_one('div#bigdata > h3').string
print(h3)





html = """ <html>
<body>
<ul id="language">
 <li id="html">HTML</li>
 <li class="script">Java Script</li>
 <li class="java">Java</li>
 <li class="spring">Spring Framework</li>
 <li class="python">Python</li>
</ul>
</body>
</html>
"""
soup = BeautifulSoup(html, 'html.parser')
print(soup.select_one('#html'))
print(soup.select_one('.script'))
print(soup.select_one('li#html'))
print(soup.select_one('li.java'))
print(soup.select_one('#language #html'))
print(soup.select_one('#language .spring'))
print(soup.select_one('ul > li#html'))
print(soup.select_one('ul > li.python'))
print(soup.select_one('ul#language > li#html'))
print(soup.select_one('ul#language > li.python'))
print(soup.select_one("li[id='html']"))
print(soup.select_one("li[class='python']"))
print(soup.select_one('li:nth-of-type(4)'))


html = """ <html>
<body>
<div id="bigdata">
 <h3>빅데이터의 요소</h3>
 <ul>
 <li>크기(Volume)</li>
 <li>다양성(Variety)</li>
 <li>속도(Velocity)</li>
 <li>정확성(Veracity)</li>
 <li>가치(Value)</li>
 </ul>
</div>
</body>
</html>
"""
soup = BeautifulSoup(html, 'html.parser')
li_list = soup.select('div#bigdata li')
for li in li_list: print(li.string)

html = """ <html>
<body>
<div id="bigdata">
 <h3>빅데이터의 요소</h3>
 <ul>
 <li>크기(Volume)</li>
 <li>다양성(Variety)</li>
 <li>속도(Velocity)</li>
 <li>정확성(Veracity)</li>
 <li>가치(Value)</li>
 </ul>
</div>
</body>
</html>
"""
soup = BeautifulSoup(html, 'html.parser')
print(soup.select('div#bigdata > h3 > ul > li'))





from bs4 import BeautifulSoup
import requests

url = "https://finance.naver.com/"
response = requests.get(url)
html = response.text
soup = BeautifulSoup(html, 'html.parser')

news = soup.select('div.news_area a')[0].string

print(news)




session_use = requests.session( )
#로그인을 위해 분석한 입력 양식의 name 속성과 값을 입력한다.
login_info = {
    "m_id": "chs9185",
    "m_passwd": "9300c2696!"
}
url_login = "http://www.hanbit.co.kr/member/login_proc.php"
response_login = session_use.post(url_login, data=login_info)
# POST 메소드로 정보를 전달하여 자동 로그인한다. response_login = session_use.post(url_login, data=login_info)
url_mypage = "http://www.hanbit.co.kr/myhanbit/myhanbit.html"
response_mypage = session_use.get(url_mypage)
html = response_mypage.text
soup = BeautifulSoup(html, 'html.parser')
print(soup)
#마일리지를 추출한다.
mileage = soup.select_one("dl.mileage_section1 span").get_text()
print('마일리지: ' + mileage)
mileage2 = soup.select_one("dl.mileage_section2 span").get_text()
print('누적 마일리지 : ' + mileage2)



from bs4 import BeautifulSoup
import requests

session_use = requests.session()
search_info = {
    "id" : "likeramen"
}
url_search = "https://pubg.op.gg/"
response_info = session_use.post(url_search, data = search_info)
url_mypage = "https://pubg.op.gg/user/" + search_info['id']

# # from urllib.parse import urljoin
# # base = "https://minhaett.net/html/a.html"
# # print(urljoin(base, 'b.html'))
# # print(urljoin(base, 'http://minhtt.com/c.html'))
# # print(urljoin(base, '//minhtt.co.kr/d.html'))                             #//만 적어도 https 뒤에 전체 내용이 바뀜.
#
# from os import makedirs
# from bs4 import  BeautifulSoup
# from urllib.request import urlretrieve
# from urllib.parse import urljoin, urlparse
# import  os.path, time, re
#
# url = "https://docs.python.org/3.6/library/"
#
# proc_files = {}                                         # HTML의 속성 타입인 빈 딕셔너리로 HTML 파일인지 판별한다.
#
# def enum_links(html, base):                             # HTML 내부에 있는 링크를 추출하고 HTMl을 분석한다.
#     soup = BeautifulSoup(html, 'html.parser')
#     links = soup.select("link[rel='stylesheet']")       # 소스보기해서 link요소의 rel값을 적어주면 된다.
#     links += soup.select('a[href]')                     # a요소의 href속성을 가져오겠다.
#     result = []
#
#     for a in links :                                    # 링크 태그의 href 속성에 적혀 있는 URL을 반복하여 추출한다.
#         href = a.attrs['href']                          # href의 요소값을 가져온다.
#         url = urljoin(base, href)                       # 기본 url에 href요소값을 추가 한다.
#         result.append(url)
#     # print('result', result)
#     return result
#
# def download_file(url):                               # 인터넷에 있는 파일을 다운하고 저장한다
#     test_url = urlparse(url)                            # URL을 6개 구성 요소로 분석 후 튜플로 반환
#     savepath = './'+test_url.netloc+test_url.path       # 6개의 구성 요소 중 netloc와 path 부분을 추가해준다.
#     if re.search(r'/$', savepath):                      # / 로 끝나면
#         savepath += 'index.html'
#     print('savepath = ', savepath)
#     savedir = os.path.dirname(savepath)                 # savepath의 디렉터리 경로를 추출하고 savedir 객체에 할당 한다.
#
#     if os.path.exists(savepath):                        # 파일 존재를 확인한다. 존재하면 True, 없으면 False
#         return savepath
#
#     if not os.path.exists(savedir):                     # 파일 존재를 확인하지 않는다.
#         print('mkdir=', savedir)
#         makedirs(savedir)                               # savepath 디렉터리 경로 추출한걸로 폴더 만들어줌.
#     try:
#         print('download=', url)
#         urlretrieve(url, savepath)                      # urlretrieve는 url와 savepath객체로 직접 파일을 다운로드 한다.
#         time.sleep(1)
#         return savepath
#     except:
#         print('다운 실패 : ', url)
#         return None
#
# def analyze_html(url, root_url):                            # 링크에 있는 것을 다운한다.
#     savepath = download_file(url)                           # download_file 함수의 인자인 url로 인터넷에 있는 파일을 다운받고 저장하며 URL을 기반으로 파일명을 결정하고 필요하다면 savepath경로로 폴더를 생성한다.
#     if savepath is None :                                   # 다운받고 저장한 것이 없는지 확인한다.
#         return
#
#     if savepath in proc_files:                              # 같은 파일이 없는지 확인한다.
#         return
#
#     proc_files[savepath] = True                             # []으로 savepath 객체로 지정하고 savepath 객체에 접근하여 True를 할당한다.
#     print('analyze_html=', url)
#
#     with open(savepath, encoding='utf-8') as f:             # 저장한 파일 호출
#         html = f.read()
#         links = enum_links(html, url)                       # savepath에서 읽은 요소값들과 받은 url 이 2개를 분석해 각 요소값들은 links에 저장시킨다(?)
#
#         for link_url in links:                              # 링크 반복
#
#             if link_url.find(root_url)!=0:                  # 루트 경로인지가 아니면!
#                 if not re.search(r'.css$', link_url):       # 문자열 끝이 .css로 끝나지 않는 문자열과 링크 경로를 검색한다.
#                     continue
#
#             if re.search(r'.(html||htm)$', link_url):       # 문자열 끝이 .html과 .htm으로 끝나는 문자열과 링크 경로를 검색한다.
#                 analyze_html(link_url, root_url)            # 조건이 참이면 url대신 link_url을 넣어.
#                 continue
#             download_file(link_url)
# analyze_html(url, url)



from bs4 import BeautifulSoup
import urllib.request as req
import  datetime

url = "https://pubg.op.gg/leaderboard/?platform=kakao&mode=tpp&queue_size=1"
html = req.urlopen(url)
soup = BeautifulSoup(html, 'html.parser')

price = soup.select_one('div.leader-board__l-nickname > a').string

change = soup.select('div.leader-board__table-content')[0].string

blind = soup.select('div.leader-board__table-content')[1].string

# exchange = datetime.date.today()
# file_name = exchange.strftime('%Y-%m-%d') + '.txt'

# with open(file_name, 'w', encoding='utf-8') as f:
#     f.write('혀재환율 가격은 {}이고 변동 가격은 {}원으로 {}ㅣㅂ니다.'.format(price, change, blind))

print('현재순위는 {}이고 sp는 {}, 경기수는{}입니다.'.format(price, change, blind))


import cx_Oracle

connection = cx_Oracle.connect("scott/tiger@localhost/xe")

cursor = connection.cursor()
cursor.execute("""
    SELECT * FROM DEPT
""")
for deptno, dname, loc in cursor:
    print("values : ", deptno, dname, loc)
    
    
from selenium import webdriver

# url = "https://www.google.co.kr/"
# driver = webdriver.Chrome(executable_path='chromedriver')                   # 기본값이 chromedriver다. 크롬드라이버를 로드하고 driver객체에 할당하는 것.
# driver.get(url)
#
# driver = webdriver.Chrome()
# driver.implicitly_wait(3)                                                   # 최대 대기 시간을 3초로 지정해서 처리가 끝날 때까지 대기한다.
# driver.get(url)
#
# driver = webdriver.Chrome()
# driver.get(url)
# driver.maximize_window()                                                    # 최대 크기로 호출

# url = "https://www.google.co.kr/"
# driver = webdriver.Chrome()
# driver.get(url)
# driver.quit()                                                                   # 브라우저와 드라이브 종료
# print('드라이버를 종료시키고 브라우저를 닫습니다.')

# driver.get(url)
# driver.close()
# print('브라우저를 닫습니다.')                                                        # 브라우저만 종료

# driver.get(url)
# driver.save_screenshot('naver_main.png')                                        # 스크린샷 저장

# driver.get(url)
# driver.get_screenshot_as_file('naver_main1.png')                                    # 스크린샷을 파일로 저장

# options = webdriver.ChromeOptions()
# options.add_argument('headless')                                                    # 여러 옵션 부여
# options.add_argument('window-size=1366x768')
# options.add_argument('disable-gpu')
# driver = webdriver.Chrome(chrome_options=options)
# print(driver)

url = "C:\pycharmtest\settest.html"                                                 # 이럴땐 절대 경로
driver = webdriver.Chrome()
driver.get(url)
id = driver.find_element_by_id('idname')                                            # id로 찾겠다
id.send_keys('minhaett')
name = driver.find_element_by_name('name')                                          # name으로 찾겠다.
name.send_keys('홍길동')


from selenium import webdriver
import time

driver = webdriver.Chrome()
driver.get('https://nid.naver.com/nidlogin.login')
# id = driver.find_element_by_name('id')
# id.send_keys('ccc9185')
# time.sleep(1)
# pw = driver.find_element_by_name('pw')
# pw.send_keys('0258a0258')
# time.sleep(1)
# driver.find_element_by_xpath('//*[@id="frmNIDLogin"]/fieldset/input').click()
id = 'ccc9185'
driver.execute_script("document.getElementsByName('id')[0].value=\'" + id + "\'")
time.sleep(1)
pw = '0258a0258'
driver.execute_script("document.getElementsByName('pw')[0].value=\'" + pw + "\'")
time.sleep(1)
driver.find_element_by_xpath('//*[@id="frmNIDLogin"]/fieldset/input').click()

from selenium import webdriver
from selenium.webdriver.common.keys import Keys

url = "http://www.gmarket.co.kr/"
driver = webdriver.Chrome()
driver.get(url)
login_bt = driver.find_element_by_class_name('sign')
login_bt.click()
id = driver.find_element_by_name('id')
id.send_keys('chs9088')
pw = driver.find_element_by_name('pwd')                                         # input name이 pwd인걸 찾아라
pw.send_keys('9123')
pw.send_keys(Keys.RETURN)
search = driver.find_element_by_class_name('sch')                               # 클래스 이름이 sch인 것을 찾아라
search.send_keys('노트북')
search.submit()
products = driver.find_elements_by_css_selector('span.title')                    # selector가 span.title인 것 모든걸 가져와라.

for product in products:
    print('-', product.text)
    
    
import urllib.request
import json
id = "xbm8iqn0cl"
pwd = "THPrwjjLOy2lab1Qmx9C9qGpqBWFiHn20LOhoj9x"
url = "https://naveropenapi.apigw.ntruss.com/map-static/v2/rasterw=300&h=300&center=127.1054221,37.3591614&level=16&X-NCP-APIGW-API-KEY-ID=xbm8iqn0cl & X-NCP-APIGW-API-KEY=THPrwjjLOy2lab1Qmx9C9qGpqBWFiHn20LOhoj9x"
print(url)


# import os
# import sys
# import urllib.request
# client_id = "xbm8iqn0cl"
# client_secret = "THPrwjjLOy2lab1Qmx9C9qGpqBWFiHn20LOhoj9x"
# encText = urllib.parse.quote("불정로 6")
# url = "https://naveropenapi.apigw.ntruss.com/map-static/v2/raster?query=" + encText # json 결과
# request = urllib.request.Request(url)
# request.add_header("X-Naver-Client-Id",client_id)
# request.add_header("X-Naver-Client-Secret",client_secret)
#
# print(url)


# url = "https://openapi.naver.com/v1/map/geocode.xml?query=" + encText # xml 결과
# request = urllib.request.Request(url)
# request.add_header("X-Naver-Client-Id",client_id)
# request.add_header("X-Naver-Client-Secret",client_secret)
# response = urllib.request.urlopen(request)
# rescode = response.getcode()
# if(rescode==200):
#     response_body = response.read()
#     print(response_body.decode('utf-8'))
# else:
#     print("Error Code:" + rescode)
#
# #json 데이터의 딕셔너리로 변환한다. data = json.loads(response.text)
# #딕셔너리를 호출한다. print(data['name'], '의 날씨는', data['weather'][0]['description'], '입니다.')


from selenium import webdriver
# import requests
import time

url = "http://www.danawa.com/"
#
# params = {
#     "query" : "손흥민",
#     "where" : "news",
#     "ie" : "utf8",
#     "sm" : "nws_hty"
# }

# response = requests.get(url, params=params)
# search = response.url
driver = webdriver.Chrome()
driver.get(url)
log_btn = driver.find_element_by_class_name('my_page_service')
log_btn.click()
id = driver.find_element_by_name('id')
id.send_keys('chs9185')
time.sleep(1)
id = driver.find_element_by_name('password')
id.send_keys('1234')
time.sleep(1)
driver.find_element_by_class_name('btn_login').click()
search = driver.find_element_by_name('k1')
search.send_keys('ASUS')
time.sleep(1)
driver.find_element_by_class_name('btn_search_submit').click()
time.sleep(10)
driver.find_element_by_xpath('//*[@id="searchOption"]/div[1]/fieldset[1]/div[2]/dl[1]/dd/ul[1]/li[2]/span/a').click()                   # 클래스가 배열 형태로 되어 있고 인덱스로 찾기가 안되서 xpath로 함.


# news = driver.find_elements_by_css_selector('div.news.mynews.section._prs_nws > ul')
#
# for new in news:
#     print(new.text)




